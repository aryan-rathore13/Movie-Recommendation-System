{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4120a0cd-0e42-4016-b6ca-809d53285f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/credits.csv in chunks...\n",
      "Loaded data/credits.csv successfully.\n",
      "Loading data/keywords.csv in chunks...\n",
      "Loaded data/keywords.csv successfully.\n",
      "Loading data/links.csv in chunks...\n",
      "Loaded data/links.csv successfully.\n",
      "Loading data/movies_metadata.csv in chunks...\n",
      "Loaded data/movies_metadata.csv successfully.\n",
      "Loading data/ratings.csv in chunks...\n",
      "Loaded data/ratings.csv successfully.\n",
      "All files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "BUCKET_NAME = 'movie-recommender-dataset'\n",
    "FILES = ['data/credits.csv', 'data/keywords.csv', 'data/links.csv', 'data/movies_metadata.csv', 'data/ratings.csv']\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "dataframes = {}\n",
    "\n",
    "for file in FILES:\n",
    "    try:\n",
    "        print(f\"Loading {file} in chunks...\")\n",
    "        obj = s3.get_object(Bucket=BUCKET_NAME, Key=file)\n",
    "        chunks = pd.read_csv(obj['Body'], chunksize=10000)  # Adjust chunksize as needed\n",
    "        dataframes[file.split('/')[-1]] = pd.concat(chunks)\n",
    "        print(f\"Loaded {file} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {file}. Error: {e}\")\n",
    "\n",
    "print(\"All files loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e746841-b08d-4c9b-bca5-67b4a38f3559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of credits.csv:\n",
      "                                                cast  \\\n",
      "0  [{'cast_id': 14, 'character': 'Woody (voice)',...   \n",
      "1  [{'cast_id': 1, 'character': 'Alan Parrish', '...   \n",
      "2  [{'cast_id': 2, 'character': 'Max Goldman', 'c...   \n",
      "3  [{'cast_id': 1, 'character': \"Savannah 'Vannah...   \n",
      "4  [{'cast_id': 1, 'character': 'George Banks', '...   \n",
      "\n",
      "                                                crew     id  \n",
      "0  [{'credit_id': '52fe4284c3a36847f8024f49', 'de...    862  \n",
      "1  [{'credit_id': '52fe44bfc3a36847f80a7cd1', 'de...   8844  \n",
      "2  [{'credit_id': '52fe466a9251416c75077a89', 'de...  15602  \n",
      "3  [{'credit_id': '52fe44779251416c91011acb', 'de...  31357  \n",
      "4  [{'credit_id': '52fe44959251416c75039ed7', 'de...  11862  \n",
      "Missing values in credits.csv:\n",
      "cast    0\n",
      "crew    0\n",
      "id      0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "First few rows of keywords.csv:\n",
      "      id                                           keywords\n",
      "0    862  [{'id': 931, 'name': 'jealousy'}, {'id': 4290,...\n",
      "1   8844  [{'id': 10090, 'name': 'board game'}, {'id': 1...\n",
      "2  15602  [{'id': 1495, 'name': 'fishing'}, {'id': 12392...\n",
      "3  31357  [{'id': 818, 'name': 'based on novel'}, {'id':...\n",
      "4  11862  [{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...\n",
      "Missing values in keywords.csv:\n",
      "id          0\n",
      "keywords    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "First few rows of links.csv:\n",
      "   movieId  imdbId   tmdbId\n",
      "0        1  114709    862.0\n",
      "1        2  113497   8844.0\n",
      "2        3  113228  15602.0\n",
      "3        4  114885  31357.0\n",
      "4        5  113041  11862.0\n",
      "Missing values in links.csv:\n",
      "movieId      0\n",
      "imdbId       0\n",
      "tmdbId     219\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "First few rows of movies_metadata.csv:\n",
      "   adult                              belongs_to_collection    budget  \\\n",
      "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
      "1  False                                                NaN  65000000   \n",
      "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
      "3  False                                                NaN  16000000   \n",
      "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
      "\n",
      "                                              genres  \\\n",
      "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
      "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
      "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
      "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
      "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
      "\n",
      "                               homepage     id    imdb_id original_language  \\\n",
      "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
      "1                                   NaN   8844  tt0113497                en   \n",
      "2                                   NaN  15602  tt0113228                en   \n",
      "3                                   NaN  31357  tt0114885                en   \n",
      "4                                   NaN  11862  tt0113041                en   \n",
      "\n",
      "                original_title  \\\n",
      "0                    Toy Story   \n",
      "1                      Jumanji   \n",
      "2             Grumpier Old Men   \n",
      "3            Waiting to Exhale   \n",
      "4  Father of the Bride Part II   \n",
      "\n",
      "                                            overview  ... release_date  \\\n",
      "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
      "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
      "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
      "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
      "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
      "\n",
      "       revenue runtime                                   spoken_languages  \\\n",
      "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
      "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
      "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
      "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
      "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
      "\n",
      "     status                                            tagline  \\\n",
      "0  Released                                                NaN   \n",
      "1  Released          Roll the dice and unleash the excitement!   \n",
      "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
      "3  Released  Friends are the people who let you be yourself...   \n",
      "4  Released  Just When His World Is Back To Normal... He's ...   \n",
      "\n",
      "                         title  video vote_average vote_count  \n",
      "0                    Toy Story  False          7.7     5415.0  \n",
      "1                      Jumanji  False          6.9     2413.0  \n",
      "2             Grumpier Old Men  False          6.5       92.0  \n",
      "3            Waiting to Exhale  False          6.1       34.0  \n",
      "4  Father of the Bride Part II  False          5.7      173.0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Missing values in movies_metadata.csv:\n",
      "adult                        0\n",
      "belongs_to_collection    40972\n",
      "budget                       0\n",
      "genres                       0\n",
      "homepage                 37684\n",
      "id                           0\n",
      "imdb_id                     17\n",
      "original_language           11\n",
      "original_title               0\n",
      "overview                   954\n",
      "popularity                   5\n",
      "poster_path                386\n",
      "production_companies         3\n",
      "production_countries         3\n",
      "release_date                87\n",
      "revenue                      6\n",
      "runtime                    263\n",
      "spoken_languages             6\n",
      "status                      87\n",
      "tagline                  25054\n",
      "title                        6\n",
      "video                        6\n",
      "vote_average                 6\n",
      "vote_count                   6\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "First few rows of ratings.csv:\n",
      "   userId  movieId  rating   timestamp\n",
      "0       1      110     1.0  1425941529\n",
      "1       1      147     4.5  1425942435\n",
      "2       1      858     5.0  1425941523\n",
      "3       1     1221     5.0  1425941546\n",
      "4       1     1246     5.0  1425941556\n",
      "Missing values in ratings.csv:\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting each DataFrame\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"First few rows of {name}:\")\n",
    "    print(df.head())            # View the top rows of each DataFrame\n",
    "    print(f\"Missing values in {name}:\")\n",
    "    print(df.isnull().sum())    # Check for missing values\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1504bce9-4a2d-4510-96b8-4d325948a469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features column created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3248/3125492592.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  movies_metadata_df = pd.read_csv(obj['Body'])\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Define the S3 bucket and file path for movies_metadata.csv\n",
    "BUCKET_NAME = 'movie-recommender-dataset'\n",
    "MOVIES_METADATA_FILE = 'data/movies_metadata.csv'\n",
    "\n",
    "# Load movies_metadata.csv from S3 into a DataFrame\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=MOVIES_METADATA_FILE)\n",
    "movies_metadata_df = pd.read_csv(obj['Body'])\n",
    "\n",
    "# Ensure that the necessary columns exist and combine them into a single feature\n",
    "if 'title' in movies_metadata_df.columns and 'overview' in movies_metadata_df.columns and 'genres' in movies_metadata_df.columns:\n",
    "    movies_metadata_df['combined_features'] = (\n",
    "        movies_metadata_df['title'].fillna('') + \" \" +\n",
    "        movies_metadata_df['overview'].fillna('') + \" \" +\n",
    "        movies_metadata_df['genres'].fillna('')\n",
    "    )\n",
    "    print(\"Combined features column created successfully!\")\n",
    "else:\n",
    "    print(\"One or more columns (title, overview, genres) are missing in movies_metadata_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43df9397-b49f-4aa2-9f5c-cb324ba6a810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (45466, 81247)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the combined_features column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_metadata_df['combined_features'])\n",
    "\n",
    "# Check the shape of the TF-IDF matrix\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d55d9d-ed0f-438c-bcb5-9bb973ff8e66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix shape: (10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Sample the first 10,000 rows\n",
    "sample_df = movies_metadata_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()  # No need to set max_features here\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_df['combined_features'])\n",
    "\n",
    "# Import necessary libraries for cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Create a sparse matrix (if not already)\n",
    "tfidf_sparse = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_sparse)\n",
    "\n",
    "# Check the shape of the cosine similarity matrix\n",
    "print(f\"Cosine similarity matrix shape: {cosine_sim.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c618d8-4375-47cb-8213-9367502ce35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recommend_movies(title, cosine_sim=cosine_sim):\n",
    "    # Check if the title exists in the DataFrame\n",
    "    if title not in sample_df['title'].values:\n",
    "        return f\"Movie titled '{title}' not found in the dataset.\"\n",
    "\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = sample_df[sample_df['title'] == title].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return sample_df.iloc[movie_indices][['title', 'overview']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3dfe57-ae90-452d-814c-93ed5e3ff608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      title                                           overview\n",
      "38293          Mother's Day  Intersecting stories with different moms colli...\n",
      "15267           Please Give  In New York City, a husband and wife butt head...\n",
      "35962            Witchcraft  A new mother and her child move into her mothe...\n",
      "42496               Dukhtar  In the mountains of Pakistan, a mother and her...\n",
      "37054     Curse of the Wolf  Dakota, a young werewolf, has finally learned ...\n",
      "39319     Adult Life Skills  Anna is stuck: sheâ€™s approaching 30 and has ju...\n",
      "20139  Girl Walk // All Day  Girl Walk // All Day is a feature-length dance...\n",
      "33903        The Diabolical  When a single mother and her two young childre...\n",
      "3438      Empire of Passion  A young man has an affair with an older woman....\n",
      "10072                Agatha  In real life, mystery writer Agatha Christie d...\n"
     ]
    }
   ],
   "source": [
    "recommendations = recommend_movies(\"Jumanji\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da3b3d0-dc2c-4876-a36d-eabf000d349f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cosine_similarity_matrix.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the TF-IDF vectorizer and cosine similarity matrix\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(cosine_sim, 'cosine_similarity_matrix.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6524dc-c6bc-4c00-9255-81726bd26ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created:\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the files were created\n",
    "print(\"Files created:\")\n",
    "print(os.path.isfile('tfidf_vectorizer.pkl'))  # Check for the TF-IDF vectorizer\n",
    "print(os.path.isfile('cosine_similarity_matrix.pkl'))  # Check for the cosine similarity matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eafe6afc-f99c-4cfe-8659-ee671d265cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded to S3 successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "BUCKET_NAME = 'movie-recommender-dataset'\n",
    "\n",
    "# Upload the saved files to S3\n",
    "s3.upload_file('tfidf_vectorizer.pkl', BUCKET_NAME, 'model/tfidf_vectorizer.pkl')\n",
    "s3.upload_file('cosine_similarity_matrix.pkl', BUCKET_NAME, 'model/cosine_similarity_matrix.pkl')\n",
    "\n",
    "print(\"Files uploaded to S3 successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da932bf-5011-4c06-840b-791725fac584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/tfidf_vectorizer.pkl.3a631aB3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m BUCKET_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie-recommender-dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Download the model files from S3\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBUCKET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/tfidf_vectorizer.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/tfidf_vectorizer.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m s3\u001b[38;5;241m.\u001b[39mdownload_file(BUCKET_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/cosine_similarity_matrix.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/cosine_similarity_matrix.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles downloaded from S3 successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/boto3/s3/inject.py:192\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/boto3/s3/transfer.py:406\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    402\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m    403\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[1;32m    404\u001b[0m )\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/tasks.py:139\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# If the task is not done (really only if some other related\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# task to the TransferFuture had failed) then execute the task's\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# main() method.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/tasks.py:162\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Log what is about to be executed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_final:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/download.py:642\u001b[0m, in \u001b[0;36mIOWriteTask._main\u001b[0;34m(self, fileobj, data, offset)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_main\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileobj, data, offset):\n\u001b[1;32m    636\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pulls off an io queue to write contents to a file\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    :param fileobj: The file handle to write content to\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    :param data: The data to write\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    :param offset: The offset to write the data to.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/utils.py:387\u001b[0m, in \u001b[0;36mDeferredOpenFile.seek\u001b[0;34m(self, where, whence)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseek\u001b[39m(\u001b[38;5;28mself\u001b[39m, where, whence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mseek(where, whence)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/utils.py:370\u001b[0m, in \u001b[0;36mDeferredOpenFile._open_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_if_needed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_byte \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_byte)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/s3transfer/utils.py:281\u001b[0m, in \u001b[0;36mOSUtils.open\u001b[0;34m(self, filename, mode)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, mode):\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/tfidf_vectorizer.pkl.3a631aB3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "BUCKET_NAME = 'movie-recommender-dataset'\n",
    "\n",
    "# Download the model files from S3\n",
    "s3.download_file(BUCKET_NAME, 'model/tfidf_vectorizer.pkl', 'model/tfidf_vectorizer.pkl')\n",
    "s3.download_file(BUCKET_NAME, 'model/cosine_similarity_matrix.pkl', 'model/cosine_similarity_matrix.pkl')\n",
    "\n",
    "print(\"Files downloaded from S3 successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00052def-7ad4-41f7-a0b7-130eccd82e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading file: Failed to upload recommend.py to your-s3-bucket-name/path/in/s3/recommend.py: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Specify the bucket name and file details\n",
    "BUCKET_NAME = 'your-s3-bucket-name'  # replace with your bucket name\n",
    "FILE_NAME = 'recommend.py'              # the file you want to upload\n",
    "OBJECT_NAME = 'path/in/s3/recommend.py' # the S3 object name (where it will be stored in the bucket)\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the file\n",
    "try:\n",
    "    s3_client.upload_file(FILE_NAME, BUCKET_NAME, OBJECT_NAME)\n",
    "    print(f\"Successfully uploaded {FILE_NAME} to s3://{BUCKET_NAME}/{OBJECT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226969e1-ec6e-4a66-868a-1a10d2834188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
